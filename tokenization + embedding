# Jude´s code for character-level tokenization

import pandas as pd
df = pd.read_csv("code_bug_fix_pairs.csv")


class CharTokenizer:
    def __init__(self, texts, indent_spaces=4):
        self.indent_spaces = indent_spaces

        # Special tokens (fixed IDs)
        self.special_tokens = ["<pad>", "<bos>", "<eos>", "<indent>"] # padding, beginning of sequence, end of sequence, indent
        self.stoi = {tok: i for i, tok in enumerate(self.special_tokens)} # map string to index
        self.itos = {i: tok for tok, i in self.stoi.items()} # map index to string

        # Collect characters
        chars = set() # set of unique characters
        for text in texts: 
            chars.update(text) # add all of the unique characters

        # Assign IDs
        offset = len(self.stoi) # offset by 4, ie numbers taken up by special tokens already
        for i, ch in enumerate(sorted(chars)): 
            self.stoi[ch] = i + offset # map the characters to index
            self.itos[i + offset] = ch # reverse map

        self.vocab_size = len(self.stoi)

    def encode(self, text, add_special_tokens=True):
        ids = []

        if add_special_tokens:
            ids.append(self.stoi["<bos>"])

        i = 0
        while i < len(text):
            # Handle indentation (only at line start)
            if text[i] == " ":
                count = 0
                while i < len(text) and text[i] == " ":
                    count += 1
                    i += 1
                # you kinda reverse engineer from the amount of spaces how many indents there are

                while count >= self.indent_spaces: # when count bigger than 4 it counts as an indent
                    ids.append(self.stoi["<indent>"]) # add token for indent
                    count -= self.indent_spaces # reduce count by 4

                # leftover spaces
                ids.extend([self.stoi[" "]] * count) # add remaining spaces as formatting spaces basically
            else:
                ids.append(self.stoi[text[i]])
                i += 1

        if add_special_tokens:
            ids.append(self.stoi["<eos>"])

        return ids

    def decode(self, ids):
        text = "" #initialize string
        for i in ids:
            token = self.itos.get(i, "") # get the token from ids (the index)
            if token == "<bos>" or token == "<eos>" or token == "<pad>":
                continue
            elif token == "<indent>":
                text += " " * self.indent_spaces #. add 4 spaces if there was an indent
            else:
                text += token #just add the token to the string
        return text


import re
def clean_code_logic(text):
    if not isinstance(text, str):
            return ""

    marker = "# Sample ID"
    index = text.find(marker)

    if index == -1:
        return text.strip()

    return text[:index].strip() 

# --- Step 1: Clean the DataFrame first ---
print("Cleaning data and building custom vocabulary...")

df['buggy_clean'] = df['buggy_code'].apply(clean_code_logic)
df['fixed_clean'] = df['fixed_code'].apply(clean_code_logic)

# --- Step 2: Gather cleaned tokens into a list ---
# Using .tolist() is much faster than iterrows()
texts = df['buggy_clean'].tolist() + df['fixed_clean'].tolist()

print(f"Collected {len(texts)} cleaned code snippets.")



tokenizer = CharTokenizer(texts)

print("Vocab size:", tokenizer.vocab_size)
print(list(tokenizer.stoi.items()))

sample = df.iloc[0]["buggy_clean"]
print("ORIGINAL:")
print(repr(sample))

encoded = tokenizer.encode(sample)
decoded = tokenizer.decode(encoded)

print("ENCODED:")
print(encoded[:50])  # print first 50 tokens
print("DECODED:")
print(repr(decoded))


assert decoded == sample
print("Yippee reversiblityy")



# PHASE 2 - Embedding + Padding
# Encode all entries 
df['buggy_entries'] = df['buggy_clean'].apply(tokenizer.encode)
df['fixed_entries'] = df['fixed_clean'].apply(tokenizer.encode)

# Step 1 - find max length
max_buggy = max(len(entry) for entry in df['buggy_entries'])
max_fixed = max(len(entry) for entry in df['fixed_entries'])
max_entry_length = max(max_buggy, max_fixed)

print(f"Max entry length is {max_entry_length}")

# Step 2 - padding function (no truncation)
# To keep it safe we will add copy of an entry
def pad_entry(entry, max_len, pad_token_id):
    entry = entry.copy()
    if len(entry) < max_len:
        entry.extend([pad_token_id] * (max_len - len(entry)))
    return entry

#Alternative with truncation
def pad_or_truncate_sequence(entry, max_len, pad_token_id):
    if len(entry) < max_len:
        entry.extend([pad_token_id] * (max_len - len(entry)))
    else:
        entry = entry[:max_len]
    return entry

# Step 3 - Pad the entries
# Lambda acts as a function of x which gives the output we define by another function
pad_token_id = tokenizer.stoi["<pad>"]
df['buggy_padded'] = df['buggy_entries'].apply(lambda x: pad_entry(x, max_entry_length, pad_token_id))
df['fixed_padded'] = df['fixed_entries'].apply(lambda x: pad_entry(x, max_entry_length, pad_token_id))

print(f"Sample padded entry: {df['buggy_padded'].iloc[0]}")

# Save padded entries
df.to_csv("code_bug_fix_pairs_tokenized_padded.csv", index=False)
print("Padded entries saved to code_bug_fix_pairs_tokenized_padded.csv")

# Step 4 - embedding layer with Keras
import tensorflow as tf
from tensorflow import keras

embedding_dim = 32  # aribitrary choice of embedding dimension
embedding_layer = keras.layers.Embedding(input_dim=tokenizer.vocab_size,
                                         output_dim=embedding_dim,
                                         input_length=max_entry_length,
                                         mask_zero=True)  # mask_zero masks the padding tokens
# Overall there are 3 dimensions: 
# 1. Batch size (number of entries processed together) TBD
# 2. Sequence length = max entry length (due to padding)
# 3. Embedding dimension (number of elements in a token vector)

# test on a sample entry
# Take first entry from dataframe, make it a tensor, and run it through embedding layer
sample_padded_entry = df['buggy_padded'].iloc[0]
sample_padded_entry_tensor = tf.constant([sample_padded_entry])  # batch = 1
embedded_output = embedding_layer(sample_padded_entry_tensor)
print("Embedded output shape:", embedded_output.shape)
print("Embedded output (first 1 token):", embedded_output[0, 0, :]) 


# PHASE 3
# Positoinal Encoding
# 2D matrix, every row is a position (there is max_length positions)
# Every position is defined as a vector of embedding dimension (32 - number of columns)
# Values of the elements in a row are defined by sine and cosine functions (sin, cos, sin...)
import numpy as np
def positional_encoding(max_len, d_model):
    pos_enc = np.zeros((max_len, d_model))
    for pos in range(max_len):
        for i in range(0, d_model, 2):
            pos_enc[pos, i] = np.sin(pos / (10000 ** ((2 * i)/d_model)))
            if i + 1 < d_model:
                pos_enc[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))
    return pos_enc

pos_enc = positional_encoding(max_entry_length, embedding_dim)
print("Positional encoding shape:", pos_enc.shape)
print("Positional encoding (first position):", pos_enc[0])

# Combine embedding with positional encoding
def add_positional_encoding(embedded_inputs, pos_enc):
    """
    embedded_inputs: 3D Tensor (batch_size, seq_len, embed_dim)
    pos_enc: 2D Matrix (seq_len, embed_dim)
    use tf.cast to convert numpy array to tensor
    we also add a new axis to this tensor to make it 3D (1, seq_len, embed_dim)
    Finally, by adding them, each row of batch gets this same positional encoding
    Update: we use "keras.layers.add" instead of "+" to preserve the mask
    """
    positional_encoding_tensor = tf.cast(pos_enc, dtype=tf.float32)
    positional_encoding_tensor = positional_encoding_tensor[tf.newaxis, ...]
    return keras.layers.add([embedded_inputs, positional_encoding_tensor])

# Combined output is a tensor of the same shape as embedded output
# However, now each value is adjusted according to its position in the sequence
combined_output = add_positional_encoding(embedded_output, pos_enc)
print("Combined output shape:", combined_output.shape)
print("Combined output (first token):", combined_output[0, 0, :])
print("Tokenization, embedding, and positional encoding complete.")

# Tranformer Block
class TransformerBlock(keras.layers.Layer):
    """
    We create a blueprint for a transfromer layer/block (containing multiple sub-layers) 
    that can be stacked up in a model of many transoformer blocks
    We combine multi-head attention which captures relationships between tokens
    and feed-forward networks which process each token individually
    We use Layer Normalization on inputs to stabilise and speed up training
    We use Dropout to prevent overfitting by randomly setting some outputs to zero.
    """

    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        """
    Arguments:
    embed_dim: Dimension of embedding vectors
    num_heads: Number of attention heads
    ff_dim: Dimension of feed-forward network
    rate: Dropout rate (percentage of neurons to drop) to prevent overfitting

    Update: make sure model supports masking by setting self.supports_masking = True

    Returns:
    1. Multi-head attention layer checks relationships between tokens in sequence
    2. Feed-forward network processes each token individually, now with context from attention
    3. Layer normalization, normalizes inputs by adjusting and scaling activations
    3.1. Adjusting and scaling activations is done by subtracting meand and dividing by std
    3.2. Epsilon is a small constant to prevent division by zero
    4. Dropout randomly sets a fraction of input units to 0 at each update during training time 
    """
        super(TransformerBlock, self).__init__()
        self.supports_masking = True
        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = keras.Sequential(
            [keras.layers.Dense(ff_dim, activation="relu"), keras.layers.Dense(embed_dim),]
        )
        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = keras.layers.Dropout(rate)
        self.dropout2 = keras.layers.Dropout(rate)

    def call(self, inputs, training, mask=None): # mask added to ignore padding tokens
        """
        Arguments:
        inputs: Combined tensor to the transformer block
        training: True/False (to train or not to train)
        
        Returns:
        Tensor of same shape as input, but each element is adjusted based on context

        Mechanism:
        1. Multi-head attention computes attention scores and weighted values,
        by comparing input to input itself (self-attention) while masking padding tokens
        2. Dropout randomly sets a fraction of input units to 0 during training
        3. First layer normalization normalizes the sum of inputs and attention output
        4. Feed-forward network processes each token individually
        5. Second dropout randomly sets a fraction of input units to 0 during training
        6. Second layer normalization normalizes the sum of the output 
        from first normalization and feed-forward output
        7. Result is a smarter tensor
        """
        attn_output = self.att(inputs, inputs, attention_mask=mask) # mask to ignore padding
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)
transformer_block = TransformerBlock(embed_dim=embedding_dim, num_heads=2, ff_dim=64)
transformer_output = transformer_block(combined_output, training=False) # don´t train yet
print("Transformer output shape:", transformer_output.shape)
print("Transformer block applied successfully.")
