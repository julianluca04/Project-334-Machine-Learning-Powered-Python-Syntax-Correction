{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7577669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self, texts, indent_spaces=4):\n",
    "        self.indent_spaces = indent_spaces\n",
    "\n",
    "        # Special tokens (fixed IDs)\n",
    "        self.special_tokens = [\"<pad>\", \"<bos>\", \"<eos>\", \"<indent>\"] # padding, beginning of sequence, end of sequence, indent\n",
    "        self.stoi = {tok: i for i, tok in enumerate(self.special_tokens)} # map string to index\n",
    "        self.itos = {i: tok for tok, i in self.stoi.items()} # map index to string\n",
    "\n",
    "        # Collect characters\n",
    "        chars = set() # set of unique characters\n",
    "        for text in texts: \n",
    "            chars.update(text) # add all of the unique characters\n",
    "\n",
    "        # Assign IDs\n",
    "        offset = len(self.stoi) # offset by 4, ie numbers taken up by special tokens already\n",
    "        for i, ch in enumerate(sorted(chars)): \n",
    "            self.stoi[ch] = i + offset # map the characters to index\n",
    "            self.itos[i + offset] = ch # reverse map\n",
    "\n",
    "        self.vocab_size = len(self.stoi)\n",
    "\n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        ids = []\n",
    "\n",
    "        if add_special_tokens:\n",
    "            ids.append(self.stoi[\"<bos>\"])\n",
    "\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            # Handle indentation (only at line start)\n",
    "            if text[i] == \" \":\n",
    "                count = 0\n",
    "                while i < len(text) and text[i] == \" \":\n",
    "                    count += 1\n",
    "                    i += 1\n",
    "                # you kinda reverse engineer from the amount of spaces how many indents there are\n",
    "\n",
    "                while count >= self.indent_spaces: # when count bigger than 4 it counts as an indent\n",
    "                    ids.append(self.stoi[\"<indent>\"]) # add token for indent\n",
    "                    count -= self.indent_spaces # reduce count by 4\n",
    "\n",
    "                # leftover spaces\n",
    "                ids.extend([self.stoi[\" \"]] * count) # add remaining spaces as formatting spaces basically\n",
    "            else:\n",
    "                ids.append(self.stoi[text[i]])\n",
    "                i += 1\n",
    "\n",
    "        if add_special_tokens:\n",
    "            ids.append(self.stoi[\"<eos>\"])\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \"\" #initialize string\n",
    "        for i in ids:\n",
    "            token = self.itos.get(i, \"\") # get the token from ids (the index)\n",
    "            if token == \"<bos>\" or token == \"<eos>\" or token == \"<pad>\":\n",
    "                continue\n",
    "            elif token == \"<indent>\":\n",
    "                text += \" \" * self.indent_spaces #. add 4 spaces if there was an indent\n",
    "            else:\n",
    "                text += token #just add the token to the string\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac3b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"code_bug_fix_pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66039bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data and building custom vocabulary...\n",
      "Collected 2000 cleaned code snippets.\n",
      "Vocab size: 49\n",
      "[('<pad>', 0), ('<bos>', 1), ('<eos>', 2), ('<indent>', 3), ('\\n', 4), (' ', 5), (\"'\", 6), ('(', 7), (')', 8), ('*', 9), ('+', 10), (',', 11), ('-', 12), ('0', 13), ('1', 14), ('2', 15), ('3', 16), ('4', 17), ('5', 18), (':', 19), ('=', 20), ('>', 21), ('F', 22), ('H', 23), ('M', 24), ('T', 25), ('[', 26), (']', 27), ('_', 28), ('a', 29), ('b', 30), ('c', 31), ('d', 32), ('e', 33), ('f', 34), ('g', 35), ('h', 36), ('i', 37), ('l', 38), ('m', 39), ('n', 40), ('o', 41), ('p', 42), ('r', 43), ('s', 44), ('t', 45), ('u', 46), ('w', 47), ('x', 48)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def clean_code_logic(text):\n",
    "    if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "    marker = \"# Sample ID\"\n",
    "    index = text.find(marker)\n",
    "\n",
    "    if index == -1:\n",
    "        return text.strip()\n",
    "\n",
    "    return text[:index].strip()\n",
    "\n",
    "# --- Step 1: Clean the DataFrame first ---\n",
    "print(\"Cleaning data and building custom vocabulary...\")\n",
    "\n",
    "df['buggy_clean'] = df['buggy_code'].apply(clean_code_logic)\n",
    "df['fixed_clean'] = df['fixed_code'].apply(clean_code_logic)\n",
    "\n",
    "# --- Step 2: Gather cleaned tokens into a list ---\n",
    "# Using .tolist() is much faster than iterrows()\n",
    "texts = df['buggy_clean'].tolist() + df['fixed_clean'].tolist()\n",
    "\n",
    "print(f\"Collected {len(texts)} cleaned code snippets.\")\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = CharTokenizer(texts)\n",
    "\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(list(tokenizer.stoi.items()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc33191f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL:\n",
      "'x = [1, 2, 3]\\nprint x'\n",
      "ENCODED:\n",
      "[1, 48, 5, 20, 5, 26, 14, 11, 5, 15, 11, 5, 16, 27, 4, 42, 43, 37, 40, 45, 5, 48, 2]\n",
      "DECODED:\n",
      "'x = [1, 2, 3]\\nprint x'\n",
      "Yippee reversiblityy\n"
     ]
    }
   ],
   "source": [
    "sample = df.iloc[0][\"buggy_clean\"]\n",
    "print(\"ORIGINAL:\")\n",
    "print(repr(sample))\n",
    "\n",
    "encoded = tokenizer.encode(sample)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(\"ENCODED:\")\n",
    "print(encoded[:50])  # print first 50 tokens\n",
    "print(\"DECODED:\")\n",
    "print(repr(decoded))\n",
    "\n",
    "\n",
    "assert decoded == sample\n",
    "print(\"Yippee reversiblityy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "783b79f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_ids: torch.Size([32, 76])\n",
      "tgt_in_ids: torch.Size([32, 76])\n",
      "tgt_out_ids: torch.Size([32, 76])\n",
      "src_x: torch.Size([32, 76, 256])\n",
      "tgt_x: torch.Size([32, 76, 256])\n",
      "src_key_padding_mask: torch.Size([32, 76])\n",
      "tgt_key_padding_mask: torch.Size([32, 76])\n",
      "tgt_causal_mask: torch.Size([76, 76])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Positional encoding + embed\n",
    "# ----------------------------\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 4096):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        T = x.size(1)\n",
    "        return x + self.pe[:T].unsqueeze(0)\n",
    "\n",
    "\n",
    "class PreTransformerInputs(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, pad_id: int, max_len: int = 4096):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.pos = SinusoidalPositionalEncoding(d_model, max_len=max_len)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        return self.pos(self.emb(input_ids))\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Masks\n",
    "# ----------------------------\n",
    "def make_key_padding_mask(input_ids: torch.Tensor, pad_id: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    nn.Transformer convention:\n",
    "      True  = this position is PAD (ignore it)\n",
    "      False = real token\n",
    "    Shape: [B, T]\n",
    "    \"\"\"\n",
    "    return (input_ids == pad_id)\n",
    "\n",
    "\n",
    "def make_causal_mask(tgt_len: int, device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    True = blocked (can't attend)\n",
    "    Shape: [T, T]\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones(tgt_len, tgt_len, dtype=torch.bool, device=device), diagonal=1)\n",
    "\n",
    "\n",
    "def pad_1d(seqs: List[List[int]], pad_id: int) -> torch.Tensor:\n",
    "    max_len = max(len(s) for s in seqs)\n",
    "    out = [s + [pad_id] * (max_len - len(s)) for s in seqs]\n",
    "    return torch.tensor(out, dtype=torch.long)\n",
    "\n",
    "\n",
    "def teacher_forcing_shift(tgt_ids_padded: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    tgt_ids_padded: [B, T]\n",
    "    returns:\n",
    "      tgt_in  [B, T-1]\n",
    "      tgt_out [B, T-1]\n",
    "    \"\"\"\n",
    "    return tgt_ids_padded[:, :-1].contiguous(), tgt_ids_padded[:, 1:].contiguous()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset using your notebook columns\n",
    "# ----------------------------\n",
    "class BugFixCharDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer):\n",
    "        self.src_texts = df[\"buggy_clean\"].astype(str).tolist()\n",
    "        self.tgt_texts = df[\"fixed_clean\"].astype(str).tolist()\n",
    "        self.tok = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # IMPORTANT:\n",
    "        # - for src: we keep <bos>/<eos> because it helps the model learn boundaries\n",
    "        # - for tgt: also keep <bos>/<eos> so we can do shifting for teacher forcing\n",
    "        src_ids = self.tok.encode(self.src_texts[idx], add_special_tokens=True)\n",
    "        tgt_ids = self.tok.encode(self.tgt_texts[idx], add_special_tokens=True)\n",
    "        return src_ids, tgt_ids\n",
    "\n",
    "\n",
    "def collate_for_transformer(batch, pad_id: int, device: torch.device):\n",
    "    \"\"\"\n",
    "    batch: list of (src_ids, tgt_ids), variable length\n",
    "    returns tensors/masks ready for transformer blocks\n",
    "    \"\"\"\n",
    "    src_list, tgt_list = zip(*batch)\n",
    "\n",
    "    src_ids = pad_1d(list(src_list), pad_id=pad_id).to(device)  # [B, Tsrc]\n",
    "    tgt_ids = pad_1d(list(tgt_list), pad_id=pad_id).to(device)  # [B, Ttgt]\n",
    "\n",
    "    tgt_in_ids, tgt_out_ids = teacher_forcing_shift(tgt_ids)     # [B, Ttgt-1]\n",
    "\n",
    "    src_key_padding_mask = make_key_padding_mask(src_ids, pad_id=pad_id)      # [B, Tsrc]\n",
    "    tgt_key_padding_mask = make_key_padding_mask(tgt_in_ids, pad_id=pad_id)   # [B, Ttgt-1]\n",
    "    tgt_causal_mask = make_causal_mask(tgt_in_ids.size(1), device=device)     # [Ttgt-1, Ttgt-1]\n",
    "\n",
    "    return {\n",
    "        \"src_ids\": src_ids,\n",
    "        \"tgt_in_ids\": tgt_in_ids,\n",
    "        \"tgt_out_ids\": tgt_out_ids,\n",
    "        \"src_key_padding_mask\": src_key_padding_mask,\n",
    "        \"tgt_key_padding_mask\": tgt_key_padding_mask,\n",
    "        \"tgt_causal_mask\": tgt_causal_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Example wiring (after you already ran your notebook cleaning + tokenizer build)\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume df already has: buggy_clean, fixed_clean\n",
    "    # and tokenizer = CharTokenizer(texts) already exists.\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    PAD_ID = tokenizer.stoi[\"<pad>\"]\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    dataset = BugFixCharDataset(df, tokenizer)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda b: collate_for_transformer(b, pad_id=PAD_ID, device=device),\n",
    "    )\n",
    "\n",
    "    # Optional: build embeddings+pos enc outputs (ready to feed into blocks)\n",
    "    d_model = 256\n",
    "    prep = PreTransformerInputs(vocab_size=vocab_size, d_model=d_model, pad_id=PAD_ID).to(device)\n",
    "\n",
    "    batch = next(iter(loader))\n",
    "\n",
    "    src_x = prep(batch[\"src_ids\"])       # [B, Tsrc, d_model]\n",
    "    tgt_x = prep(batch[\"tgt_in_ids\"])    # [B, Ttgt-1, d_model]\n",
    "\n",
    "    print(\"src_ids:\", batch[\"src_ids\"].shape)\n",
    "    print(\"tgt_in_ids:\", batch[\"tgt_in_ids\"].shape)\n",
    "    print(\"tgt_out_ids:\", batch[\"tgt_out_ids\"].shape)\n",
    "    print(\"src_x:\", src_x.shape)\n",
    "    print(\"tgt_x:\", tgt_x.shape)\n",
    "    print(\"src_key_padding_mask:\", batch[\"src_key_padding_mask\"].shape)\n",
    "    print(\"tgt_key_padding_mask:\", batch[\"tgt_key_padding_mask\"].shape)\n",
    "    print(\"tgt_causal_mask:\", batch[\"tgt_causal_mask\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922c1ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
