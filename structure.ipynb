{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import pandas as pd\n",
    "import re\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jude's code: tokenizer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data and building custom vocabulary...\n",
      "Collected 2000 cleaned code snippets.\n",
      "Vocab size: 49\n",
      "[('<pad>', 0), ('<bos>', 1), ('<eos>', 2), ('<indent>', 3), ('\\n', 4), (' ', 5), (\"'\", 6), ('(', 7), (')', 8), ('*', 9), ('+', 10), (',', 11), ('-', 12), ('0', 13), ('1', 14), ('2', 15), ('3', 16), ('4', 17), ('5', 18), (':', 19), ('=', 20), ('>', 21), ('F', 22), ('H', 23), ('M', 24), ('T', 25), ('[', 26), (']', 27), ('_', 28), ('a', 29), ('b', 30), ('c', 31), ('d', 32), ('e', 33), ('f', 34), ('g', 35), ('h', 36), ('i', 37), ('l', 38), ('m', 39), ('n', 40), ('o', 41), ('p', 42), ('r', 43), ('s', 44), ('t', 45), ('u', 46), ('w', 47), ('x', 48)]\n",
      "\n",
      "ORIGINAL:\n",
      "'x = [1, 2, 3]\\nprint x'\n",
      "\n",
      "ENCODED (First 50 tokens):\n",
      "[1, 48, 5, 20, 5, 26, 14, 11, 5, 15, 11, 5, 16, 27, 4, 42, 43, 37, 40, 45, 5, 48, 2]\n",
      "\n",
      "DECODED:\n",
      "'x = [1, 2, 3]\\nprint x'\n",
      "\n",
      "Yippee reversiblityy\n"
     ]
    }
   ],
   "source": [
    "class CharTokenizer: #convert raw text into numbers (encoding) and numbers back into text (decoding)\n",
    "    def __init__(self, texts, indent_spaces=4): # creates dictionary where every unique character is assigned a unique number\n",
    "        self.indent_spaces = indent_spaces\n",
    "\n",
    "        # Special tokens (fixed IDs)\n",
    "        self.special_tokens = [\"<pad>\", \"<bos>\", \"<eos>\", \"<indent>\"] # padding, beginning of sequence, end of sequence, indent\n",
    "        self.stoi = {tok: i for i, tok in enumerate(self.special_tokens)} # map string to index\n",
    "        self.itos = {i: tok for tok, i in self.stoi.items()} # map index to string\n",
    "\n",
    "        # Collect characters\n",
    "        chars = set() # set of unique characters\n",
    "        for text in texts: \n",
    "            chars.update(text) # add all of the unique characters\n",
    "\n",
    "        # Assign IDs\n",
    "        offset = len(self.stoi) # offset by 4, ie numbers taken up by special tokens already\n",
    "        for i, ch in enumerate(sorted(chars)): \n",
    "            self.stoi[ch] = i + offset # map the characters to index\n",
    "            self.itos[i + offset] = ch # reverse map\n",
    "\n",
    "        self.vocab_size = len(self.stoi)\n",
    "\n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        ids = []\n",
    "\n",
    "        if add_special_tokens:\n",
    "            ids.append(self.stoi[\"<bos>\"])\n",
    "\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            # Handle indentation (only at line start)\n",
    "            if text[i] == \" \":\n",
    "                count = 0\n",
    "                while i < len(text) and text[i] == \" \":\n",
    "                    count += 1\n",
    "                    i += 1\n",
    "                # you kinda reverse engineer from the amount of spaces how many indents there are\n",
    "\n",
    "                while count >= self.indent_spaces: # when count bigger than 4 it counts as an indent\n",
    "                    ids.append(self.stoi[\"<indent>\"]) # add token for indent\n",
    "                    count -= self.indent_spaces # reduce count by 4\n",
    "\n",
    "                # leftover spaces\n",
    "                ids.extend([self.stoi[\" \"]] * count) # add remaining spaces as formatting spaces basically\n",
    "            else:\n",
    "                ids.append(self.stoi[text[i]])\n",
    "                i += 1\n",
    "\n",
    "        if add_special_tokens:\n",
    "            ids.append(self.stoi[\"<eos>\"])\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \"\" #initialize string\n",
    "        for i in ids:\n",
    "            token = self.itos.get(i, \"\") # get the token from ids (the index)\n",
    "            if token == \"<bos>\" or token == \"<eos>\" or token == \"<pad>\":\n",
    "                continue\n",
    "            elif token == \"<indent>\":\n",
    "                text += \" \" * self.indent_spaces # add 4 spaces if there was an indent\n",
    "            else:\n",
    "                text += token # just add the token to the string\n",
    "        return text\n",
    "\n",
    "def clean_code_logic(text):\n",
    "    if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "    marker = \"# Sample ID\" # search for specific marker in the code to remove anything after it\n",
    "    index = text.find(marker)\n",
    "\n",
    "    if index == -1:\n",
    "        return text.strip()\n",
    "\n",
    "    return text[:index].strip()\n",
    "\n",
    "# --- Execution ---\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"code_bug_fix_pairs.csv\")\n",
    "\n",
    "# Step 1: Clean the DataFrame first\n",
    "print(\"Cleaning data and building custom vocabulary...\")\n",
    "\n",
    "df['buggy_clean'] = df['buggy_code'].apply(clean_code_logic)\n",
    "df['fixed_clean'] = df['fixed_code'].apply(clean_code_logic)\n",
    "\n",
    "# Step 2: Gather cleaned tokens into a list\n",
    "texts = df['buggy_clean'].tolist() + df['fixed_clean'].tolist()\n",
    "print(f\"Collected {len(texts)} cleaned code snippets.\")\n",
    "\n",
    "# Initialize Tokenizer\n",
    "tokenizer = CharTokenizer(texts)\n",
    "\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(list(tokenizer.stoi.items()))\n",
    "\n",
    "# Step 3: Test Reversibility\n",
    "sample = df.iloc[0][\"buggy_clean\"]\n",
    "print(\"\\nORIGINAL:\")\n",
    "print(repr(sample))\n",
    "\n",
    "encoded = tokenizer.encode(sample)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(\"\\nENCODED (First 50 tokens):\")\n",
    "print(encoded[:50]) \n",
    "\n",
    "print(\"\\nDECODED:\")\n",
    "print(repr(decoded))\n",
    "\n",
    "assert decoded == sample\n",
    "print(\"\\nYippee reversiblityy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data for an encoder-decoder model: create a processing pipeline that uses the tokenizer to generate 3 specific sequences that model requires.\n",
    "Use Teacher Forcing: give decoder correct answer shifted by one char so it learns to predict the next correct char based on encoder's context.\n",
    "\n",
    "Unlike traditional deep neural networks where each dense layer has distinct weight matrices. RNNs use shared weights across time steps, allowing them to remember information over sequences.\n",
    "\n",
    "Positional encoding is added so that the transformer knows the order of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Encoder Input: (1000, 128)\n",
      "Shape of Decoder Input: (1000, 128)\n",
      "Shape of Decoder Target: (1000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Configuration for training\n",
    "MAX_LEN = 128 # Max number of characters per snippet\n",
    "\n",
    "def prepare_seq2seq_data(df, tokenizer, max_len):\n",
    "    encoder_input_data = []\n",
    "    decoder_input_data = []\n",
    "    decoder_target_data = []\n",
    "\n",
    "    for _, row in df.iterrows(): # iterate through each row in the dataframe and ignore the index nb\n",
    "        # 1. Encoder Input: The Buggy Code\n",
    "        buggy_ids = tokenizer.encode(row['buggy_clean'])\n",
    "        \n",
    "        # 2. Decoder Input: The Fixed Code (starts with <bos>)\n",
    "        fixed_ids = tokenizer.encode(row['fixed_clean'])\n",
    "        \n",
    "        # Split fixed_ids for Teacher Forcing\n",
    "        dec_in = fixed_ids[:-1] # Input to decoder: <bos>, 'd', 'e', 'f' ... (excludes last token)\n",
    "        dec_tar = fixed_ids[1:] # Target for decoder: 'd', 'e', 'f' ... <eos> (excludes first token)\n",
    "\n",
    "        encoder_input_data.append(buggy_ids)\n",
    "        decoder_input_data.append(dec_in)\n",
    "        decoder_target_data.append(dec_tar)\n",
    "\n",
    "    # Pad everything so the Tensors have a consistent shape\n",
    "    encoder_input_data = pad_sequences(encoder_input_data, maxlen=max_len, padding='post')\n",
    "    decoder_input_data = pad_sequences(decoder_input_data, maxlen=max_len, padding='post')\n",
    "    decoder_target_data = pad_sequences(decoder_target_data, maxlen=max_len, padding='post')\n",
    "\n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data\n",
    "\n",
    "# Process the data\n",
    "encoder_in, decoder_in, decoder_target = prepare_seq2seq_data(df, tokenizer, MAX_LEN)\n",
    "\n",
    "print(\"Shape of Encoder Input:\", encoder_in.shape)  # (Samples, 128)\n",
    "print(\"Shape of Decoder Input:\", decoder_in.shape)  # (Samples, 128)\n",
    "print(\"Shape of Decoder Target:\", decoder_target.shape) # (Samples, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding process: turn integer IDs into rich, high-dimensional vectors that represent the meaning of the characters. We use an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m en_char_emb = layers.Embedding(vocab_size, embed_dim)(encoder_inputs)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 2. Positional Embedding\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# We create a range of numbers [0, 1, 2, ..., seq_len]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m en_pos_indices = tf.range(start=\u001b[32m0\u001b[39m, limit=\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_inputs\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m], delta=\u001b[32m1\u001b[39m)\n\u001b[32m     12\u001b[39m en_pos_emb = layers.Embedding(input_dim=MAX_LEN, output_dim=embed_dim)(en_pos_indices)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 3. Combine them (Add)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/backend/common/keras_tensor.py:194\u001b[39m, in \u001b[36mKerasTensor.__tf_tensor__\u001b[39m\u001b[34m(self, dtype, name)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    195\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    196\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    197\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mused when constructing Keras Functional models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    198\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    200\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mand `keras.ops`). \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    204\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    208\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    209\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    210\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    211\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    212\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    213\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    214\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size # vocabulary size\n",
    "embed_dim = 128 # embedding dimension\n",
    "latent_dim = 256 # internal representation dimension\n",
    "\n",
    "# encoder reads buggy code\n",
    "encoder_inputs = layers.Input(shape=(None,), name=\"buggy_input\") # symbolic placeholder = no data, just a promise that there will be, a shape\n",
    "en_char_emb = layers.Embedding(vocab_size, embed_dim)(encoder_inputs)\n",
    "\n",
    "# 2. Positional Embedding\n",
    "# We create a range of numbers [0, 1, 2, ..., seq_len]\n",
    "en_pos_indices = tf.range(start=0, limit=tf.shape(encoder_inputs)[1], delta=1)\n",
    "en_pos_emb = layers.Embedding(input_dim=MAX_LEN, output_dim=embed_dim)(en_pos_indices)\n",
    "\n",
    "# 3. Combine them (Add)\n",
    "en_emb = en_char_emb + en_pos_emb\n",
    "\n",
    "# using bidirectional GRU so the model sees context from both directions\n",
    "encoder_outputs, state_fwd, state_back = layers.Bidirectional(\n",
    "    layers.GRU(latent_dim, return_state=True))(en_emb)\n",
    "# combine forward and backward states into one \"thought\" vector\n",
    "encoder_state = layers.Concatenate()([state_fwd, state_back])\n",
    "\n",
    "# decoder writes fixed code\n",
    "decoder_inputs = layers.Input(shape=(None,), name=\"fixed_input_target\")\n",
    "de_char_emb = layers.Embedding(vocab_size, embed_dim)(decoder_inputs)\n",
    "\n",
    "# 2. Positional Embedding\n",
    "de_pos_indices = tf.range(start=0, limit=tf.shape(decoder_inputs)[1], delta=1) # problem here bc trying to do real math on symbolic placeholder\n",
    "de_pos_emb = layers.Embedding(input_dim=MAX_LEN, output_dim=embed_dim)(de_pos_indices)\n",
    "\n",
    "# 3. Combine them\n",
    "de_emb = de_char_emb + de_pos_emb\n",
    "\n",
    "# decoder GRU uses encoder's state as its initial \"memory\"\n",
    "decoder_gru = layers.GRU(latent_dim * 2, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _ = decoder_gru(de_emb, initial_state=encoder_state)\n",
    "\n",
    "# attention layer \n",
    "# allows the decoder to \"look back\" at the encoder's output\n",
    "attention = layers.Attention()([decoder_outputs, en_emb])\n",
    "concat = layers.Concatenate()([decoder_outputs, attention])\n",
    "\n",
    "# output layer: predicts the probability of the next character\n",
    "output = layers.Dense(vocab_size, activation=\"softmax\")(concat)\n",
    "\n",
    "# define full model\n",
    "model = tf.keras.Model([encoder_inputs, decoder_inputs], output)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
