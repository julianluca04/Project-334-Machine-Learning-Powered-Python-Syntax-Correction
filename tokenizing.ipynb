{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "e6977a1ac429bb60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T08:42:14.777513Z",
     "start_time": "2026-01-13T08:42:13.822202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!jupyter nbconvert --to html tokenizing.ipynb\n",
    "!open tokenizing.html\n"
   ],
   "id": "776236971482585",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook tokenizing.ipynb to html\r\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/nbconvert/filters/highlight.py:71: UserWarning: IPython3 lexer unavailable, falling back on Python 3\r\n",
      "  return _pygments_highlight(\r\n",
      "[NbConvertApp] Writing 360122 bytes to tokenizing.html\r\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T07:29:18.405309Z",
     "start_time": "2026-01-13T07:29:18.356482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"code_bug_fix_pairs.csv\")\n",
    "data.head()"
   ],
   "id": "f4337b0ccf02448b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   id                                         buggy_code  \\\n",
       "0   1             x = [1, 2, 3]\\nprint x\\n# Sample ID: 1   \n",
       "1   2  list = [1, 2, 3, 4]\\nfor i in list\\n    print(...   \n",
       "2   3  def factorial(n):\\n    if n == 1\\n        retu...   \n",
       "3   4  def foo()\\n    print('Missing colon in functio...   \n",
       "4   5  def factorial(n):\\n    if n == 1\\n        retu...   \n",
       "\n",
       "                                          fixed_code  \\\n",
       "0            x = [1, 2, 3]\\nprint(x)\\n# Sample ID: 1   \n",
       "1  lst = [1, 2, 3, 4]\\nfor i in lst:\\n    print(i...   \n",
       "2  def factorial(n):\\n    if n == 1:\\n        ret...   \n",
       "3  def foo():\\n    print('Fixed missing colon in ...   \n",
       "4  def factorial(n):\\n    if n == 1:\\n        ret...   \n",
       "\n",
       "                                 commit_message  \\\n",
       "0  Improved readability with proper indentation   \n",
       "1        Corrected conditional operator mistake   \n",
       "2             Resolved off-by-one error in loop   \n",
       "3  Added missing parentheses for print function   \n",
       "4          Fixed bug in recursive function call   \n",
       "\n",
       "                                          commit_url        date  \n",
       "0  https://github.com/open-source-repo/commit/a5a...  2024-12-16  \n",
       "1  https://github.com/open-source-repo/commit/f47...  2024-01-03  \n",
       "2  https://github.com/open-source-repo/commit/e89...  2023-09-05  \n",
       "3  https://github.com/open-source-repo/commit/bd7...  2024-09-15  \n",
       "4  https://github.com/open-source-repo/commit/d66...  2024-01-24  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>buggy_code</th>\n",
       "      <th>fixed_code</th>\n",
       "      <th>commit_message</th>\n",
       "      <th>commit_url</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>x = [1, 2, 3]\\nprint x\\n# Sample ID: 1</td>\n",
       "      <td>x = [1, 2, 3]\\nprint(x)\\n# Sample ID: 1</td>\n",
       "      <td>Improved readability with proper indentation</td>\n",
       "      <td>https://github.com/open-source-repo/commit/a5a...</td>\n",
       "      <td>2024-12-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>list = [1, 2, 3, 4]\\nfor i in list\\n    print(...</td>\n",
       "      <td>lst = [1, 2, 3, 4]\\nfor i in lst:\\n    print(i...</td>\n",
       "      <td>Corrected conditional operator mistake</td>\n",
       "      <td>https://github.com/open-source-repo/commit/f47...</td>\n",
       "      <td>2024-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>def factorial(n):\\n    if n == 1\\n        retu...</td>\n",
       "      <td>def factorial(n):\\n    if n == 1:\\n        ret...</td>\n",
       "      <td>Resolved off-by-one error in loop</td>\n",
       "      <td>https://github.com/open-source-repo/commit/e89...</td>\n",
       "      <td>2023-09-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>def foo()\\n    print('Missing colon in functio...</td>\n",
       "      <td>def foo():\\n    print('Fixed missing colon in ...</td>\n",
       "      <td>Added missing parentheses for print function</td>\n",
       "      <td>https://github.com/open-source-repo/commit/bd7...</td>\n",
       "      <td>2024-09-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>def factorial(n):\\n    if n == 1\\n        retu...</td>\n",
       "      <td>def factorial(n):\\n    if n == 1:\\n        ret...</td>\n",
       "      <td>Fixed bug in recursive function call</td>\n",
       "      <td>https://github.com/open-source-repo/commit/d66...</td>\n",
       "      <td>2024-01-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenising",
   "id": "596c4d8c74d807b2"
  },
  {
   "cell_type": "code",
   "id": "7577669e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T07:27:32.138473Z",
     "start_time": "2026-01-13T07:27:32.127342Z"
    }
   },
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self, txt, indent_spaces=4):\n",
    "        self.indent_spaces = indent_spaces\n",
    "\n",
    "        # Special tokens (fixed IDs)\n",
    "        self.special_tokens = [\"<pad>\", \"<bos>\", \"<eos>\", \"<indent>\",\"<sep>\"] # padding, beginning of sequence, end of sequence, indent\n",
    "        self.str = {tok: i for i, tok in enumerate(self.special_tokens)} # map string to index\n",
    "        self.itm = {i: tok for tok, i in self.str.items()} # map index to string\n",
    "\n",
    "        # Collect characters\n",
    "        chars = set() # set of unique characters\n",
    "        for text in txt:\n",
    "            chars.update(text) # add all the unique characters\n",
    "\n",
    "        # Assign IDs\n",
    "        offset = len(self.str) # offset by 4, ie numbers taken up by special tokens already\n",
    "        for i, ch in enumerate(sorted(chars)): \n",
    "            self.str[ch] = i + offset # map the characters to index\n",
    "            self.itm[i + offset] = ch # reverse map\n",
    "\n",
    "        self.vocab_size = len(self.str)\n",
    "\n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        ids = []\n",
    "\n",
    "        if add_special_tokens:\n",
    "            ids.append(self.str[\"<bos>\"])\n",
    "\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            # Handle indentation (only at line start)\n",
    "            if text[i] == \" \":\n",
    "                count = 0\n",
    "                while i < len(text) and text[i] == \" \":\n",
    "                    count += 1\n",
    "                    i += 1\n",
    "                # you kinda reverse engineer from the amount of spaces how many indents there are\n",
    "\n",
    "                while count >= self.indent_spaces: # when count bigger than 4 it counts as an indent\n",
    "                    ids.append(self.str[\"<indent>\"]) # add token for indent\n",
    "                    count -= self.indent_spaces # reduce count by 4\n",
    "\n",
    "                # leftover spaces\n",
    "                ids.extend([self.str[\" \"]] * count) # add remaining spaces as formatting spaces basically\n",
    "            else:\n",
    "                ids.append(self.str[text[i]])\n",
    "                i += 1\n",
    "\n",
    "        if add_special_tokens:\n",
    "            ids.append(self.str[\"<eos>\"])\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \"\" #initialize string\n",
    "        for i in ids:\n",
    "            token = self.itm.get(i, \"\") # get the token from ids (the index)\n",
    "            if token == \"<bos>\" or token == \"<eos>\" or token == \"<pad>\":\n",
    "                continue\n",
    "            elif token == \"<indent>\":\n",
    "                text += \" \" * self.indent_spaces #. add 4 spaces if there was an indent\n",
    "            else:\n",
    "                text += token #just add the token to the string\n",
    "        return text\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "5ac3b494",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T07:27:35.545882Z",
     "start_time": "2026-01-13T07:27:35.171611Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"code_bug_fix_pairs.csv\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "66039bd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T07:39:40.422852Z",
     "start_time": "2026-01-13T07:39:40.392015Z"
    }
   },
   "source": [
    "\n",
    "def clean_code_logic(text):\n",
    "    if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "    marker = \"# Sample ID\"\n",
    "    index = text.find(marker)\n",
    "\n",
    "    if index == -1:\n",
    "        return text.strip()\n",
    "\n",
    "    return text[:index].strip()\n",
    "\n",
    "# --- Step 1: Clean the DataFrame first ---\n",
    "print(\"Cleaning data and building custom vocabulary...\")\n",
    "\n",
    "df['buggy_clean'] = df['buggy_code'].apply(clean_code_logic)\n",
    "df['fixed_clean'] = df['fixed_code'].apply(clean_code_logic)\n",
    "\n",
    "# --- Step 2: Gather cleaned tokens into a list ---\n",
    "# Using .tolist() is much faster than iter rows()\n",
    "texts = df['buggy_clean'].tolist() + df['fixed_clean'].tolist()\n",
    "\n",
    "print(f\"Collected {len(texts)} cleaned code snippets.\")\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = CharTokenizer(texts)\n",
    "\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(list(tokenizer.str.items()))\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data and building custom vocabulary...\n",
      "Collected 2000 cleaned code snippets.\n",
      "Vocab size: 50\n",
      "[('<pad>', 0), ('<bos>', 1), ('<eos>', 2), ('<indent>', 3), ('<sep>', 4), ('\\n', 5), (' ', 6), (\"'\", 7), ('(', 8), (')', 9), ('*', 10), ('+', 11), (',', 12), ('-', 13), ('0', 14), ('1', 15), ('2', 16), ('3', 17), ('4', 18), ('5', 19), (':', 20), ('=', 21), ('>', 22), ('F', 23), ('H', 24), ('M', 25), ('T', 26), ('[', 27), (']', 28), ('_', 29), ('a', 30), ('b', 31), ('c', 32), ('d', 33), ('e', 34), ('f', 35), ('g', 36), ('h', 37), ('i', 38), ('l', 39), ('m', 40), ('n', 41), ('o', 42), ('p', 43), ('r', 44), ('s', 45), ('t', 46), ('u', 47), ('w', 48), ('x', 49)]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "cc33191f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T07:27:40.283715Z",
     "start_time": "2026-01-13T07:27:40.249650Z"
    }
   },
   "source": [
    "sample = df.iloc[0][\"buggy_clean\"]\n",
    "print(\"ORIGINAL:\")\n",
    "print(repr(sample))\n",
    "\n",
    "encoded = tokenizer.encode(sample)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(\"ENCODED:\")\n",
    "print(encoded[:50])  # print first 50 tokens\n",
    "print(\"DECODED:\")\n",
    "print(repr(decoded))\n",
    "\n",
    "\n",
    "assert decoded == sample\n",
    "print(\"Decodes tokens back into text!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL:\n",
      "'x = [1, 2, 3]\\nprint x'\n",
      "ENCODED:\n",
      "[1, 48, 5, 20, 5, 26, 14, 11, 5, 15, 11, 5, 16, 27, 4, 42, 43, 37, 40, 45, 5, 48, 2]\n",
      "DECODED:\n",
      "'x = [1, 2, 3]\\nprint x'\n",
      "Decodes tokens back into text!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Embeddings Layer\n",
   "id": "84748e8ce9464d42"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T08:59:11.624162Z",
     "start_time": "2026-01-13T08:59:11.573888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(df.columns.tolist())\n",
    "df.head(3)\n"
   ],
   "id": "482b0302537b30ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'buggy_code', 'fixed_code', 'commit_message', 'commit_url', 'date', 'buggy_clean', 'fixed_clean']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   id                                         buggy_code  \\\n",
       "0   1             x = [1, 2, 3]\\nprint x\\n# Sample ID: 1   \n",
       "1   2  list = [1, 2, 3, 4]\\nfor i in list\\n    print(...   \n",
       "2   3  def factorial(n):\\n    if n == 1\\n        retu...   \n",
       "\n",
       "                                          fixed_code  \\\n",
       "0            x = [1, 2, 3]\\nprint(x)\\n# Sample ID: 1   \n",
       "1  lst = [1, 2, 3, 4]\\nfor i in lst:\\n    print(i...   \n",
       "2  def factorial(n):\\n    if n == 1:\\n        ret...   \n",
       "\n",
       "                                 commit_message  \\\n",
       "0  Improved readability with proper indentation   \n",
       "1        Corrected conditional operator mistake   \n",
       "2             Resolved off-by-one error in loop   \n",
       "\n",
       "                                          commit_url        date  \\\n",
       "0  https://github.com/open-source-repo/commit/a5a...  2024-12-16   \n",
       "1  https://github.com/open-source-repo/commit/f47...  2024-01-03   \n",
       "2  https://github.com/open-source-repo/commit/e89...  2023-09-05   \n",
       "\n",
       "                                         buggy_clean  \\\n",
       "0                             x = [1, 2, 3]\\nprint x   \n",
       "1   list = [1, 2, 3, 4]\\nfor i in list\\n    print(i)   \n",
       "2  def factorial(n):\\n    if n == 1\\n        retu...   \n",
       "\n",
       "                                         fixed_clean  \n",
       "0                            x = [1, 2, 3]\\nprint(x)  \n",
       "1    lst = [1, 2, 3, 4]\\nfor i in lst:\\n    print(i)  \n",
       "2  def factorial(n):\\n    if n == 1:\\n        ret...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>buggy_code</th>\n",
       "      <th>fixed_code</th>\n",
       "      <th>commit_message</th>\n",
       "      <th>commit_url</th>\n",
       "      <th>date</th>\n",
       "      <th>buggy_clean</th>\n",
       "      <th>fixed_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>x = [1, 2, 3]\\nprint x\\n# Sample ID: 1</td>\n",
       "      <td>x = [1, 2, 3]\\nprint(x)\\n# Sample ID: 1</td>\n",
       "      <td>Improved readability with proper indentation</td>\n",
       "      <td>https://github.com/open-source-repo/commit/a5a...</td>\n",
       "      <td>2024-12-16</td>\n",
       "      <td>x = [1, 2, 3]\\nprint x</td>\n",
       "      <td>x = [1, 2, 3]\\nprint(x)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>list = [1, 2, 3, 4]\\nfor i in list\\n    print(...</td>\n",
       "      <td>lst = [1, 2, 3, 4]\\nfor i in lst:\\n    print(i...</td>\n",
       "      <td>Corrected conditional operator mistake</td>\n",
       "      <td>https://github.com/open-source-repo/commit/f47...</td>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>list = [1, 2, 3, 4]\\nfor i in list\\n    print(i)</td>\n",
       "      <td>lst = [1, 2, 3, 4]\\nfor i in lst:\\n    print(i)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>def factorial(n):\\n    if n == 1\\n        retu...</td>\n",
       "      <td>def factorial(n):\\n    if n == 1:\\n        ret...</td>\n",
       "      <td>Resolved off-by-one error in loop</td>\n",
       "      <td>https://github.com/open-source-repo/commit/e89...</td>\n",
       "      <td>2023-09-05</td>\n",
       "      <td>def factorial(n):\\n    if n == 1\\n        retu...</td>\n",
       "      <td>def factorial(n):\\n    if n == 1:\\n        ret...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T09:52:04.133333Z",
     "start_time": "2026-01-13T09:52:04.104748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "d_model = 256\n",
    "\n",
    "PAD_ID = tokenizer.str[\"<pad>\"]\n",
    "BOS_ID = tokenizer.str[\"<bos>\"]\n",
    "EOS_ID = tokenizer.str[\"<eos>\"]\n",
    "\n",
    "# ADD THIS TOKEN IN YOUR TOKENIZER SPECIAL TOKENS\n",
    "SEP_ID = tokenizer.str[\"<sep>\"]  # must exist\n",
    "\n",
    "token_embedding = nn.Embedding(\n",
    "    num_embeddings=tokenizer.vocab_size,\n",
    "    embedding_dim=d_model,\n",
    "    padding_idx=PAD_ID\n",
    ")\n",
    "\n",
    "# Optional: force PAD row to zeros\n",
    "with torch.no_grad():\n",
    "    token_embedding.weight[PAD_ID].zero_()\n",
    "\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Embedding dim:\", d_model)\n",
    "print(\"PAD_ID:\", PAD_ID, \"SEP_ID:\", SEP_ID)\n",
    "\n",
    "# ---- USE REAL DATASET HERE ----\n",
    "sample_buggy = df.iloc[0][\"buggy_code\"]\n",
    "sample_clean = df.iloc[0][\"fixed_code\"]\n",
    "\n",
    "buggy_ids = tokenizer.encode(sample_buggy, add_special_tokens=False)\n",
    "clean_ids = tokenizer.encode(sample_clean, add_special_tokens=False)\n",
    "\n",
    "# Build decoder-only correction sequence: <bos> buggy <sep> clean <eos>\n",
    "ids = [BOS_ID] + buggy_ids + [SEP_ID] + clean_ids + [EOS_ID]\n",
    "\n",
    "x = torch.tensor(ids, dtype=torch.long).unsqueeze(0)  # (1, T)\n",
    "emb = token_embedding(x)\n",
    "\n",
    "print(\"Sequence length:\", len(ids))\n",
    "print(\"x shape:\", x.shape)\n",
    "print(\"emb shape:\", emb.shape)\n",
    "print(emb[0, :5, :8])\n",
    "\n",
    "# Check sep exists and roughly where it is\n",
    "sep_pos = (x[0] == SEP_ID).nonzero(as_tuple=True)[0].item()\n",
    "print(\"SEP position:\", sep_pos, \"=> buggy_len:\", len(buggy_ids), \"clean_len:\", len(clean_ids))\n",
    "print(\"Expected sep_pos:\", 1 + len(buggy_ids), \"Actual:\", sep_pos)\n",
    "\n",
    "\n",
    "pad_norm = token_embedding.weight[PAD_ID].norm().item()\n",
    "print(\"PAD embedding norm:\", pad_norm)"
   ],
   "id": "419726837e7db3ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 50\n",
      "Embedding dim: 256\n",
      "PAD_ID: 0 SEP_ID: 4\n",
      "Sequence length: 68\n",
      "x shape: torch.Size([1, 68])\n",
      "emb shape: torch.Size([1, 68, 256])\n",
      "tensor([[ 1.0598, -0.4031, -1.7674,  0.2216,  0.4563,  0.7601,  1.5383,  0.1208],\n",
      "        [ 0.9320, -0.3950, -0.3423,  1.1899, -0.0314,  0.3191,  2.6929, -0.7372],\n",
      "        [-0.0220, -1.2752,  0.8734,  0.3941, -0.5526, -0.0518, -0.7748,  0.9775],\n",
      "        [ 1.3041, -1.1076, -0.1412,  0.0630,  1.7751, -0.2507,  1.2804,  1.7896],\n",
      "        [-0.0220, -1.2752,  0.8734,  0.3941, -0.5526, -0.0518, -0.7748,  0.9775]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "SEP position: 33 => buggy_len: 32 clean_len: 33\n",
      "Expected sep_pos: 33 Actual: 33\n",
      "PAD embedding norm: 0.0\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T09:05:52.712879Z",
     "start_time": "2026-01-13T09:05:52.686436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "PAD_ID = tokenizer.str[\"<pad>\"]\n",
    "BOS_ID = tokenizer.str[\"<bos>\"]\n",
    "EOS_ID = tokenizer.str[\"<eos>\"]\n",
    "SEP_ID = tokenizer.str[\"<sep>\"]\n",
    "\n",
    "class SyntaxCorrectionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=256, buggy_col=\"buggy_clean\", fixed_col=\"fixed_clean\"):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.buggy_col = buggy_col\n",
    "        self.fixed_col = fixed_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        buggy = self.df.iloc[idx][self.buggy_col]\n",
    "        fixed = self.df.iloc[idx][self.fixed_col]\n",
    "\n",
    "        buggy_ids = self.tok.encode(buggy, add_special_tokens=False)\n",
    "        fixed_ids = self.tok.encode(fixed, add_special_tokens=False)\n",
    "\n",
    "        ids = [BOS_ID] + buggy_ids + [SEP_ID] + fixed_ids + [EOS_ID]\n",
    "\n",
    "        # truncate\n",
    "        ids = ids[: self.max_len]\n",
    "\n",
    "        # labels: ignore everything up to SEP (inclusive)\n",
    "        labels = ids.copy()\n",
    "        if SEP_ID in ids:\n",
    "            sep_pos = ids.index(SEP_ID)\n",
    "            labels[:sep_pos + 1] = [-100] * (sep_pos + 1)\n",
    "        else:\n",
    "            labels = [-100] * len(ids)\n",
    "\n",
    "        return torch.tensor(ids, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "#below function pads variable-length sequences, masks padding tokens, and prevents padded positions from contributing to the loss\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids, labels = zip(*batch)\n",
    "\n",
    "    input_ids = rnn.pad_sequence(input_ids, batch_first=True, padding_value=PAD_ID)\n",
    "    labels = rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    attention_mask = (input_ids != PAD_ID).long()  # 1=real, 0=pad\n",
    "    return input_ids, labels, attention_mask\n",
    "\n",
    "train_ds = SyntaxCorrectionDataset(df, tokenizer, max_len=256)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "\n",
    "# --- sanity check one batch ---\n",
    "input_ids, labels, attention_mask = next(iter(train_loader))\n",
    "\n",
    "print(\"input_ids:\", input_ids.shape)\n",
    "print(\"labels:\", labels.shape)\n",
    "print(\"attention_mask:\", attention_mask.shape)\n",
    "\n",
    "# Verify: labels ignored before SEP for the first item\n",
    "first = 0\n",
    "sep_positions = (input_ids[first] == SEP_ID).nonzero(as_tuple=True)[0]\n",
    "print(\"SEP positions (first item):\", sep_positions.tolist())\n",
    "\n",
    "if len(sep_positions) > 0:\n",
    "    s = sep_positions[0].item()\n",
    "    print(\"Ignored count up to SEP:\", (labels[first, :s+1] == -100).sum().item(), \"expected\", s+1)\n",
    "    print(\"First supervised label index:\", (labels[first] != -100).nonzero(as_tuple=True)[0][0].item())\n"
   ],
   "id": "971d2b05c01104e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([16, 152])\n",
      "labels: torch.Size([16, 152])\n",
      "attention_mask: torch.Size([16, 152])\n",
      "SEP positions (first item): [42]\n",
      "Ignored count up to SEP: 43 expected 43\n",
      "First supervised label index: 43\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "continue with mode.forward()",
   "id": "67b25fea199ec69b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
