{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "7577669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self, texts, indent_spaces=4):\n",
    "        self.indent_spaces = indent_spaces\n",
    "\n",
    "        # Special tokens (fixed IDs)\n",
    "        self.special_tokens = [\"<pad>\", \"<bos>\", \"<eos>\", \"<indent>\"] # padding, beginning of sequence, end of sequence, indent\n",
    "        self.stoi = {tok: i for i, tok in enumerate(self.special_tokens)} # map string to index\n",
    "        self.itos = {i: tok for tok, i in self.stoi.items()} # map index to string\n",
    "\n",
    "        # Collect characters\n",
    "        chars = set() # set of unique characters\n",
    "        for text in texts: \n",
    "            chars.update(text) # add all of the unique characters\n",
    "\n",
    "        # Assign IDs\n",
    "        offset = len(self.stoi) # offset by 4, ie numbers taken up by special tokens already\n",
    "        for i, ch in enumerate(sorted(chars)): \n",
    "            self.stoi[ch] = i + offset # map the characters to index\n",
    "            self.itos[i + offset] = ch # reverse map\n",
    "\n",
    "        self.vocab_size = len(self.stoi)\n",
    "\n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        ids = []\n",
    "\n",
    "        if add_special_tokens:\n",
    "            ids.append(self.stoi[\"<bos>\"])\n",
    "\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            # Handle indentation (only at line start)\n",
    "            if text[i] == \" \":\n",
    "                count = 0\n",
    "                while i < len(text) and text[i] == \" \":\n",
    "                    count += 1\n",
    "                    i += 1\n",
    "                # you kinda reverse engineer from the amount of spaces how many indents there are\n",
    "\n",
    "                while count >= self.indent_spaces: # when count bigger than 4 it counts as an indent\n",
    "                    ids.append(self.stoi[\"<indent>\"]) # add token for indent\n",
    "                    count -= self.indent_spaces # reduce count by 4\n",
    "\n",
    "                # leftover spaces\n",
    "                ids.extend([self.stoi[\" \"]] * count) # add remaining spaces as formatting spaces basically\n",
    "            else:\n",
    "                ids.append(self.stoi[text[i]])\n",
    "                i += 1\n",
    "\n",
    "        if add_special_tokens:\n",
    "            ids.append(self.stoi[\"<eos>\"])\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \"\" #initialize string\n",
    "        for i in ids:\n",
    "            token = self.itos.get(i, \"\") # get the token from ids (the index)\n",
    "            if token == \"<bos>\" or token == \"<eos>\" or token == \"<pad>\":\n",
    "                continue\n",
    "            elif token == \"<indent>\":\n",
    "                text += \" \" * self.indent_spaces #. add 4 spaces if there was an indent\n",
    "            else:\n",
    "                text += token #just add the token to the string\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "5ac3b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"code_bug_fix_pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "66039bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data and building custom vocabulary...\n",
      "Collected 2000 cleaned code snippets.\n",
      "Vocab size: 49\n",
      "[('<pad>', 0), ('<bos>', 1), ('<eos>', 2), ('<indent>', 3), ('\\n', 4), (' ', 5), (\"'\", 6), ('(', 7), (')', 8), ('*', 9), ('+', 10), (',', 11), ('-', 12), ('0', 13), ('1', 14), ('2', 15), ('3', 16), ('4', 17), ('5', 18), (':', 19), ('=', 20), ('>', 21), ('F', 22), ('H', 23), ('M', 24), ('T', 25), ('[', 26), (']', 27), ('_', 28), ('a', 29), ('b', 30), ('c', 31), ('d', 32), ('e', 33), ('f', 34), ('g', 35), ('h', 36), ('i', 37), ('l', 38), ('m', 39), ('n', 40), ('o', 41), ('p', 42), ('r', 43), ('s', 44), ('t', 45), ('u', 46), ('w', 47), ('x', 48)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def clean_code_logic(text):\n",
    "    if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "    marker = \"# Sample ID\"\n",
    "    index = text.find(marker)\n",
    "\n",
    "    if index == -1:\n",
    "        return text.strip()\n",
    "\n",
    "    return text[:index].strip()\n",
    "\n",
    "# --- Step 1: Clean the DataFrame first ---\n",
    "print(\"Cleaning data and building custom vocabulary...\")\n",
    "\n",
    "df['buggy_clean'] = df['buggy_code'].apply(clean_code_logic)\n",
    "df['fixed_clean'] = df['fixed_code'].apply(clean_code_logic)\n",
    "\n",
    "# --- Step 2: Gather cleaned tokens into a list ---\n",
    "# Using .tolist() is much faster than iterrows()\n",
    "texts = df['buggy_clean'].tolist() + df['fixed_clean'].tolist()\n",
    "\n",
    "print(f\"Collected {len(texts)} cleaned code snippets.\")\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = CharTokenizer(texts)\n",
    "\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(list(tokenizer.stoi.items()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "cc33191f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL:\n",
      "'x = [1, 2, 3]\\nprint x'\n",
      "ENCODED:\n",
      "[1, 48, 5, 20, 5, 26, 14, 11, 5, 15, 11, 5, 16, 27, 4, 42, 43, 37, 40, 45, 5, 48, 2]\n",
      "DECODED:\n",
      "'x = [1, 2, 3]\\nprint x'\n",
      "Yippee reversiblityy\n"
     ]
    }
   ],
   "source": [
    "sample = df.iloc[0][\"buggy_clean\"]\n",
    "print(\"ORIGINAL:\")\n",
    "print(repr(sample))\n",
    "\n",
    "encoded = tokenizer.encode(sample)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(\"ENCODED:\")\n",
    "print(encoded[:50])  # print first 50 tokens\n",
    "print(\"DECODED:\")\n",
    "print(repr(decoded))\n",
    "\n",
    "\n",
    "assert decoded == sample\n",
    "print(\"Yippee reversiblityy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7c43ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range (0,len(token_ids) - max_length, stride):\n",
    "           input_chunk = token_ids[i:i+max_length]\n",
    "           target_chunk = token_ids[i+1:i+max_length+1]\n",
    "\n",
    "           self.input_ids.append(torch.tensor(input_chunk))\n",
    "           self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "06826c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt,batch_size = 4,max_length =256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
    "    tokenizer = CharTokenizer(texts)\n",
    "    \n",
    "    dataset = GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last,num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7e0a8c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      " tensor([[ 1, 48,  5, 20],\n",
      "        [ 5, 26, 14, 11],\n",
      "        [ 5, 15, 11,  5],\n",
      "        [16, 27,  4, 42],\n",
      "        [43, 37, 40, 45]])\n",
      "targets\n",
      " tensor([[48,  5, 20,  5],\n",
      "        [26, 14, 11,  5],\n",
      "        [15, 11,  5, 16],\n",
      "        [27,  4, 42, 43],\n",
      "        [37, 40, 45,  5]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(df.iloc[0][\"buggy_clean\"], batch_size=8, max_length=4, stride=4,shuffle=False,drop_last=False,num_workers=0,)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs,targets = next(data_iter)\n",
    "print(\"inputs\\n\",inputs)\n",
    "print(\"targets\\n\",targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3c44f01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.3035,  ..., -0.0315, -1.0640,  0.9417],\n",
      "        [-1.3152, -0.0677, -0.1350,  ..., -0.4840, -0.2713, -0.0774],\n",
      "        [ 0.5229,  0.1553,  0.5247,  ..., -0.4098,  0.4978, -0.3721],\n",
      "        ...,\n",
      "        [ 0.3152, -0.7450, -0.9576,  ..., -0.8263,  0.1970,  0.5988],\n",
      "        [-0.8103, -0.3605, -0.7001,  ...,  0.4064, -0.4669,  0.4912],\n",
      "        [ 0.6639, -2.1862,  0.0922,  ...,  1.2086, -0.4484,  0.0459]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "output_dim = 256 * 2\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "dd2358ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      " tensor([[ 1, 48,  5, 20],\n",
      "        [ 5, 26, 14, 11],\n",
      "        [ 5, 15, 11,  5],\n",
      "        [16, 27,  4, 42]])\n",
      "targets\n",
      " tensor([[48,  5, 20,  5],\n",
      "        [26, 14, 11,  5],\n",
      "        [15, 11,  5, 16],\n",
      "        [27,  4, 42, 43]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(df.iloc[0][\"buggy_clean\"], batch_size=4, max_length=max_length, stride=max_length,shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"inputs\\n\",inputs)\n",
    "print(\"targets\\n\",targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "5780fedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 512])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = embedding_layer(inputs)\n",
    "token_embeddings.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
